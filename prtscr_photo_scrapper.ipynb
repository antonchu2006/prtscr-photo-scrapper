{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prtscr-photo-scrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7RlHt-805P8"
      },
      "source": [
        "!pip install requests\n",
        "!pip install python-csv\n",
        "!pip install pysocks\n",
        "!pip install beautifulsoup4\n",
        "!pip install lxml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwDva3WY1Jy8"
      },
      "source": [
        "#!/usr/bin/python3\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import random\n",
        "import concurrent.futures\n",
        "import string\n",
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import sys\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "try:\n",
        "    os.mkdir(\"/content/drive/MyDrive/output_prtscr\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
        "\n",
        "def scrape(cap_num):\n",
        "\n",
        "    print(\"Scrapping...\")\n",
        "    \n",
        "    scraped_num = 0\n",
        "    while cap_num > scraped_num:\n",
        "        try:\n",
        "            slug = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(6))\n",
        "            url = \"https://prnt.sc/\" + slug\n",
        "            try:\n",
        "                response = requests.get(url, headers=headers, timeout=2)\n",
        "            except:\n",
        "                pass\n",
        "            content = response.content.decode()\n",
        "            soup = BeautifulSoup(content, features='lxml')\n",
        "            ufr = requests.get(soup.img['src'], headers=headers)\n",
        "            f = open(f'/content/drive/MyDrive/output_prtscr/{slug}.png', 'wb')\n",
        "            f.write(ufr.content)\n",
        "            f.close()\n",
        "            print(f'[+] Received file {slug}.png')\n",
        "            scraped_num += 1\n",
        "        except requests.exceptions.MissingSchema:\n",
        "            pass\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    \n",
        "\n",
        "  n_of_links = int(input(\"Number of links for scrapping: \"))\n",
        "  scrape(n_of_links)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}